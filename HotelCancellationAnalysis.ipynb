{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0534e-c527-490e-8d1a-22b7f0bd3160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import findspark and initialize. \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e83253-a215-4fa3-86d2-7c3c2beb4a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import CSV file utilizing PySpark\n",
    "path = 'Resources/INNHotelsGroup.csv'\n",
    "spark.sparkContext.addFile(path)\n",
    "df = spark.read.csv(path, header=True, sep=',')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807cfbe-5688-4965-ba26-e32e05303285",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = df.toPandas()\n",
    "\n",
    "# Data Cleaning\n",
    "meal_plan_dummies = pd.get_dummies(pd_df[\"type_of_meal_plan\"])\n",
    "meal_plan_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4c3cc-5036-4bab-bb3a-0a9ae60c371f",
   "metadata": {},
   "outputs": [],
   "source": [
    "room_type_dummies = pd.get_dummies(pd_df[\"room_type_reserved\"])\n",
    "room_type_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e24985-67fa-4eda-8e78-4d3ff6b1a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pd.concat([pd_df, meal_plan_dummies, room_type_dummies], axis=1)\n",
    "cleaned_df = cleaned_df.drop(columns=[\"type_of_meal_plan\", \"room_type_reserved\"])\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd4872-e205-43aa-931b-2d7119d95d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace string values with boolean values to make data easier to use\n",
    "def encode_market(market):\n",
    "    if market == \"Online\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# Call the encode_market function on the market column\n",
    "cleaned_df[\"market_segment_type\"] = cleaned_df[\"market_segment_type\"].apply(encode_market)\n",
    "cleaned_df.head()\n",
    "\n",
    "def encode_cancel(cancel):\n",
    "    if cancel == \"Canceled\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# Call the encode_cancel function on the cancel column\n",
    "cleaned_df[\"booking_status\"] = cleaned_df[\"booking_status\"].apply(encode_cancel)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125443a4-0ca6-4c2d-a4ff-66f74bf0309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index of cleaned_df\n",
    "cleaned_df.set_index(\"Booking_ID\", inplace=True)\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d04d4-e6ac-470b-acc8-5672bb1f8a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies for Machine Learning Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a02ef-c96d-45fa-ae9e-9a06ce55feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = cleaned_df['booking_status'].values\n",
    "X = cleaned_df.drop(columns='booking_status').values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943cf97-40b6-4694-8f95-db53a44455c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find shape of the data to determine best number nodes for the model\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dbb7f-fb08-4c15-8f13-c25fea0319fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476089bf-b5c0-4308-924c-ee4f3717bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3698e1-0f17-4545-ac2d-46cdab2ccb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 7\n",
    "#hidden_nodes_layer2 = 3\n",
    "\n",
    "nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "#nn_model.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8632aed-4d93-4b6b-850d-f4d5b999511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06767c-2c94-4466-bc46-818916cf5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "fit_model = nn_model.fit(X_train_scaled,y_train,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3619a46-1c79-4446-a49f-58783989978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52be33f-a7be-4747-b67f-5f4edb1436f1",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cf1bd-a555-40ad-b2b2-572bb3a228da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the LogisticRegression module from SKLearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Instantiate the Logistic Regression model\n",
    "# Assign a random_state parameter of 1 to the model\n",
    "logistic_regression_model = LogisticRegression(random_state=1)\n",
    "\n",
    "# Fit the model using training data\n",
    "lr_model = logistic_regression_model.fit(X_train, y_train)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11c8e9-48ec-40f7-9625-1bc2424e1c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using the testing data\n",
    "testing_predictions = lr_model.predict(X_test)\n",
    "testing_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac4bb4-0730-4679-a14a-3f0ea39770fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the balanced_accuracy score of the model\n",
    "balanced_accuracy = balanced_accuracy_score(y_test, testing_predictions)\n",
    "print(f\"Balanced Accuracy Score : {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc85f9-8ef7-4bc8-9f5c-cee6593083db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a confusion matrix for the model\n",
    "cm = confusion_matrix(y_test, testing_predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9ab8d-00a3-4608-8d8b-661eabad730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report for the model\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, testing_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78ceb2-2dc6-4d35-ae9a-09b4efe8ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28d5588-1933-479a-b24e-3af01a19aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies for Machine Learning Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79357d89-d8e3-40e8-87fd-f54fcd280f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = cleaned_df['booking_status'].values\n",
    "X = cleaned_df.copy()\n",
    "X.drop(\"booking_status\", axis=1, inplace=True)\n",
    "X.head()\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5af74-86ae-4f44-ad5b-ba4bc6148be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fitting Standard Scaller\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06195a-9270-4c09-8ace-a0fc0a823e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=500, random_state=78)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions using the testing data\n",
    "predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4b45f-9b2f-494e-bcbb-984c36651b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"]\n",
    ")\n",
    "\n",
    "# Calculating the accuracy score\n",
    "acc_score = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4fd04-8752-4d3a-9bdb-e2ca26e2d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests in sklearn will automatically calculate feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "# We can sort the features by their importance\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da629bdd-44c7-434a-bf5f-41c752fe9e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df = pd.DataFrame(sorted(zip(rf_model.feature_importances_, X.columns), reverse=True))\n",
    "importances_df.set_index(importances_df[1], inplace=True)\n",
    "importances_df.drop(columns=1, inplace=True)\n",
    "importances_df.rename(columns={0: 'Feature Importances'}, inplace=True)\n",
    "importances_sorted = importances_df.sort_values(by='Feature Importances')\n",
    "importances_sorted.plot(kind='barh', color='lightgreen', title= 'Features Importances', legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b9641-9069-4c28-a148-00f9685782ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
